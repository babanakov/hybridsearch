{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.13.3-py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastembed\n",
      "  Downloading fastembed-0.6.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[K     |████████████████████████████████| 275 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio-tools>=1.41.0\n",
      "  Downloading grpcio_tools-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.41.0\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.26.14\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic>=1.10.8\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[K     |████████████████████████████████| 431 kB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx[http2]>=0.20.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 6.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting mmh3<6.0.0,>=4.1.0\n",
      "  Downloading mmh3-5.1.0-cp39-cp39-macosx_11_0_arm64.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0,>=2.31\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<1.0,>=0.15\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting loguru<0.8.0,>=0.7.2\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting onnxruntime<1.20.0,>=1.17.0\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-macosx_11_0_universal2.whl (16.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.8 MB 9.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.20\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow<12.0.0,>=10.3.0\n",
      "  Downloading pillow-11.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-rust-stemmers<0.2.0,>=0.1.0\n",
      "  Downloading py_rust_stemmers-0.1.5-cp39-cp39-macosx_11_0_arm64.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0,>=4.66\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 32.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 66.5 MB 69 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (58.0.4)\n",
      "Collecting protobuf<6.0dev,>=5.26.1\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[K     |████████████████████████████████| 417 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 11.4 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h2<5,>=3\n",
      "  Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hyperframe<7,>=6.1\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Collecting hpack<5,>=4.1\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 91.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 38.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from anyio->httpx[http2]>=0.20.0->qdrant-client) (1.2.1)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, sniffio, requests, pyyaml, h11, fsspec, filelock, numpy, mpmath, MarkupSafe, hyperframe, humanfriendly, huggingface-hub, httpcore, hpack, anyio, tokenizers, threadpoolctl, sympy, scipy, safetensors, regex, pydantic-core, protobuf, networkx, joblib, jinja2, httpx, h2, grpcio, flatbuffers, coloredlogs, annotated-types, transformers, torch, scikit-learn, pydantic, py-rust-stemmers, portalocker, pillow, onnxruntime, mmh3, loguru, grpcio-tools, sentence-transformers, qdrant-client, fastembed\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 coloredlogs-15.0.1 fastembed-0.6.0 filelock-3.18.0 flatbuffers-25.2.10 fsspec-2025.3.0 grpcio-1.71.0 grpcio-tools-1.71.0 h11-0.14.0 h2-4.2.0 hpack-4.1.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.29.3 humanfriendly-10.0 hyperframe-6.1.0 idna-3.10 jinja2-3.1.6 joblib-1.4.2 loguru-0.7.3 mmh3-5.1.0 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 onnxruntime-1.19.2 pillow-11.1.0 portalocker-2.10.1 protobuf-5.29.4 py-rust-stemmers-0.1.5 pydantic-2.10.6 pydantic-core-2.27.2 pyyaml-6.0.2 qdrant-client-1.13.3 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sentence-transformers-3.4.1 sniffio-1.3.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.50.0 urllib3-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install qdrant-client fastembed sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:29<00:00,  5.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "\n",
    "# Enable Metal GPU acceleration\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.set_per_process_memory_fraction(0.75)  # Prevent OOM\n",
    "\n",
    "# Generate synthetic dataset\n",
    "documents = [\n",
    "    f\"Product review {i}: Features include {random.choice(['wireless', '4K', 'OLED', 'smart'])} tech\"\n",
    "    for i in range(1_000_000)\n",
    "]\n",
    "\n",
    "# Initialize models with GPU optimizations\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device).half()  # FP16 precision\n",
    "sparse_model = SparseTextEmbedding( model_name=\"prithivida/Splade_PP_en_v1\", providers=[\"metal\"]  # Metal-accelerated ONNX \n",
    "                                   )\n",
    "\n",
    "# Configure batch processing parameters\n",
    "batch_size = 1024  # Optimal for M3's GPU architecture\n",
    "data_points = []\n",
    "\n",
    "# Add a progress bar using tqdm\n",
    "with tqdm(total=len(documents), desc=\"Data Points Progress\") as progress_bar:\n",
    "    for batch_idx in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[batch_idx:batch_idx + batch_size]\n",
    "        \n",
    "        # Generate dense embeddings (GPU-accelerated)\n",
    "        with torch.inference_mode(), torch.autocast(device_type=device):\n",
    "            dense_vectors = dense_model.encode(\n",
    "                batch_docs,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "        \n",
    "        # Generate sparse embeddings (Metal-optimized)\n",
    "        sparse_vectors = list(sparse_model.embed(\n",
    "            batch_docs,\n",
    "            batch_size=batch_size\n",
    "        ))\n",
    "        \n",
    "        # Batch processing for payloads\n",
    "        batch_user_ids = [f\"user_{random.randint(1,10)}\" for _ in batch_docs]\n",
    "        \n",
    "        # Format for Qdrant and update progress bar\n",
    "        for i, (doc, dense_vec, sparse_vec) in enumerate(zip(batch_docs, dense_vectors, sparse_vectors)):\n",
    "            data_points.append({\n",
    "                \"id\": batch_idx + i,\n",
    "                \"vector\": {\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"sparse\": {\n",
    "                        \"indices\": sparse_vec.indices.tolist(),\n",
    "                        \"values\": sparse_vec.values.tolist()\n",
    "                    }\n",
    "                },\n",
    "                \"payload\": {\n",
    "                    \"text\": doc,\n",
    "                    \"user_id\": batch_user_ids[i]\n",
    "                }\n",
    "            })\n",
    "            progress_bar.update(1)  # Increment progress bar by 1 for each data point created\n",
    "\n",
    "# Optional: Force GPU memory cleanup\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/babanakov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/babanakov/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'qdrant_client.models' has no attribute 'GpuResourceConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqdrant_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QdrantClient, models\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m QdrantClient(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6333\u001b[39m)\n\u001b[1;32m      5\u001b[0m client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m      6\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhybrid-search-demo-gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     vectors_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m: models\u001b[38;5;241m.\u001b[39mVectorParams(\n\u001b[1;32m      9\u001b[0m             size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m     10\u001b[0m             distance\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistance\u001b[38;5;241m.\u001b[39mCOSINE,\n\u001b[1;32m     11\u001b[0m             hnsw_config\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mHnswConfigDiff(\n\u001b[1;32m     12\u001b[0m                 gpu_indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable GPU acceleration\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m                 gpu_resources\u001b[38;5;241m=\u001b[39m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGpuResourceConfig\u001b[49m(\n\u001b[1;32m     14\u001b[0m                     device_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     15\u001b[0m                     memory_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m  \u001b[38;5;66;03m# 4GB GPU memory allocation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                 )\n\u001b[1;32m     17\u001b[0m             )\n\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m     },\n\u001b[1;32m     20\u001b[0m     sparse_vectors_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m: models\u001b[38;5;241m.\u001b[39mSparseVectorParams(\n\u001b[1;32m     22\u001b[0m             index\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSparseIndexParams(\n\u001b[1;32m     23\u001b[0m                 on_disk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Keep in GPU memory\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                 gpu_acceleration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m             )\n\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m     },\n\u001b[1;32m     28\u001b[0m     optimizers_config\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mOptimizersConfigDiff(\n\u001b[1;32m     29\u001b[0m         indexing_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[1;32m     30\u001b[0m         memmap_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,       \n\u001b[1;32m     31\u001b[0m         search_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m     32\u001b[0m     ),\n\u001b[1;32m     33\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     34\u001b[0m     replication_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'qdrant_client.models' has no attribute 'GpuResourceConfig'"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"hybrid-search-demo-gpu\",\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=768,\n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=models.HnswConfigDiff(\n",
    "                gpu_indexing=True,  # Enable GPU acceleration\n",
    "                gpu_resources=models.GpuResourceConfig(\n",
    "                    device_id=0,\n",
    "                    memory_limit=4096  # 4GB GPU memory allocation\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,  # Keep in GPU memory\n",
    "                gpu_acceleration=True\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=20000,\n",
    "        memmap_threshold=20000,       \n",
    "        search_threshold=20000\n",
    "    ),\n",
    "    shard_number=3,\n",
    "    replication_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dense_vec, sparse_vec, payload in data_points:\n",
    "    client.upsert(\n",
    "        collection_name=\"hybrid-search-demo\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=idx,\n",
    "                vector={\"dense\": dense_vec, \"sparse\": sparse_vec},\n",
    "                payload=payload,\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert import Searcher\n",
    "\n",
    "# Initialize ColBERT for reranking\n",
    "colbert_searcher = Searcher(index=\"colbertv2.0\")\n",
    "\n",
    "def hybrid_search(query, user_filter=None):\n",
    "    # Generate query embeddings\n",
    "    dense_query_vec = dense_model.encode(query)\n",
    "    sparse_query_vec = sparse_model.embed(query)\n",
    "\n",
    "    # Perform hybrid search in Qdrant\n",
    "    results = client.search_batch(\n",
    "        collection_name=\"hybrid-search-demo\",\n",
    "        requests=[\n",
    "            models.SearchRequest(\n",
    "                vector=models.NamedVector(name=\"dense\", vector=dense_query_vec),\n",
    "                filter=models.Filter(\n",
    "                    must=[models.FieldCondition(key=\"user_id\", match=user_filter)]\n",
    "                ) if user_filter else None,\n",
    "                limit=100,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "            models.SearchRequest(\n",
    "                vector=models.NamedSparseVector(name=\"sparse\", vector=sparse_query_vec),\n",
    "                filter=models.Filter(\n",
    "                    must=[models.FieldCondition(key=\"user_id\", match=user_filter)]\n",
    "                ) if user_filter else None,\n",
    "                limit=100,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine results (reciprocal rank fusion)\n",
    "    fused_results = reciprocal_rank_fusion(results[0], results[1])\n",
    "\n",
    "    # Rerank results using ColBERT\n",
    "    reranked_results = colbert_searcher.rerank(query, [res.payload[\"text\"] for res in fused_results])\n",
    "    \n",
    "    return reranked_results[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.fusion import reciprocal_rank_fusion\n",
    "\n",
    "def hybrid_search(query: str, user_id: str):\n",
    "    # Generate GPU-accelerated query vectors\n",
    "    query_dense = list(dense_model.query_embed(query))[0]\n",
    "    query_sparse = list(sparse_model.query_embed(query))[0]\n",
    "    \n",
    "    # Parallel GPU-accelerated searches\n",
    "    results = client.search_batch(\n",
    "        collection_name=\"hybrid-gpu-demo\",\n",
    "        requests=[\n",
    "            models.SearchRequest(\n",
    "                vector=models.NamedVector(\n",
    "                    name=\"dense\",\n",
    "                    vector=query_dense.tolist()\n",
    "                ),\n",
    "                filter=models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_id)\n",
    "                ),\n",
    "                limit=100\n",
    "            ),\n",
    "            models.SearchRequest(\n",
    "                vector=models.NamedSparseVector(\n",
    "                    name=\"sparse\",\n",
    "                    vector=query_sparse.as_object()\n",
    "                ),\n",
    "                filter=models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_id)\n",
    "                ),\n",
    "                limit=100\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # GPU-accelerated reranking\n",
    "    fused = reciprocal_rank_fusion(\n",
    "        results[0],\n",
    "        results[1],\n",
    "        method=\"gpu\"  # Use Metal-accelerated RRF\n",
    "    )\n",
    "    \n",
    "    return fused[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Sample query about product features\"\n",
    "user_filter = {\"value\": \"user_5\"}  # Example filter by user_id\n",
    "\n",
    "results = hybrid_search(query=query, user_filter=user_filter)\n",
    "\n",
    "for idx, (score, text) in enumerate(results):\n",
    "    print(f\"{idx + 1}. [Score: {score:.2f}] {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
