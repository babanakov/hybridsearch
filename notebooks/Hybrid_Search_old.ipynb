{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install qdrant-client fastembed sentence-transformers numpy pandas pyarrow colbert accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer warnings\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # M3 GPU fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Load Seattle Police data from official source\n",
    "url = \"~/Downloads/SPD_Crime_Data__2008-Present.csv\"\n",
    "# url = \"https://data.seattle.gov/api/views/tazs-3rd5/rows.csv?accessType=DOWNLOAD\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Column names to use\n",
    "crime_type_col = 'NIBRS Offense Code Description'\n",
    "subcategory_col = 'Offense Sub Category'\n",
    "location_col = 'Neighborhood'\n",
    "date_col = 'Offense Date'\n",
    "precinct_col = 'Precinct'\n",
    "\n",
    "# Filter and prepare documents from police reports\n",
    "df = df.dropna(subset=[crime_type_col, subcategory_col]).reset_index(drop=True)\n",
    "documents = [\n",
    "    f\"Police report {i}: {row[crime_type_col]} - {row[subcategory_col]} \" \n",
    "    f\"at {row[location_col]} on {pd.to_datetime(row[date_col]).strftime('%Y-%m-%d')}\"\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Memory optimization: Clear unused objects\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# Initialize models with GPU optimizations\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.set_per_process_memory_fraction(0.65)  # Increased safety margin\n",
    "\n",
    "# Optimized model configuration\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "dense_model = dense_model.half().eval()  # FP16 + inference mode\n",
    "\n",
    "# Switch to CoreML-compatible sparse model\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    providers=[\"CoreMLExecutionProvider\"],\n",
    "    provider_options={\n",
    "        \"MLComputeUnits\": \"CPUAndGPU\",\n",
    "        \"RequireStaticInputShapes\": \"0\",\n",
    "        \"EnableOnSubgraphs\": \"1\"\n",
    "    },\n",
    "    quantize=True  # 4-bit quantization\n",
    ")\n",
    "\n",
    "# Batch processing configuration (reduced for 8GB RAM)\n",
    "batch_size = 512  # Reduced from 1024\n",
    "data_points = []\n",
    "total_documents = len(documents)\n",
    "print(f\"\\nTotal documents to process: {total_documents}\")\n",
    "\n",
    "for idx in range(0, total_documents, batch_size):\n",
    "    batch_docs = documents[idx:idx+batch_size]\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for micro_batch in np.array_split(batch_docs, 4):\n",
    "        # Generate embeddings with memory cleanup\n",
    "        with torch.inference_mode(), torch.autocast(device_type=device):\n",
    "            dense_vectors = dense_model.encode(micro_batch, convert_to_numpy=True)\n",
    "            sparse_vectors = list(sparse_model.embed(micro_batch))\n",
    "        \n",
    "        # Format for Qdrant\n",
    "        for i, (doc, dense_vec, sparse_vec) in enumerate(zip(micro_batch, dense_vectors, sparse_vectors)):\n",
    "            data_points.append({\n",
    "                \"id\": idx + i,\n",
    "                \"vector\": {\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"sparse\": {\n",
    "                        \"indices\": sparse_vec.indices.tolist(),\n",
    "                        \"values\": sparse_vec.values.tolist()\n",
    "                    }\n",
    "                },\n",
    "                \"payload\": {\n",
    "                    \"text\": doc,\n",
    "                    \"user_id\": f\"user_{(idx + i) % 10 + 1}\"  # Simplified user assignment\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        # Explicit memory cleanup\n",
    "        del dense_vectors, sparse_vectors\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Update status\n",
    "    sys.stdout.write(f\"\\rProcessed: {len(data_points)}/{total_documents} | Batch size: {batch_size} | Mem usage: {torch.mps.current_allocated_memory()/1e6:.1f}MB\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n\\nProcessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Initialize models with GPU optimizations\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.set_per_process_memory_fraction(0.65)  # Increased safety margin\n",
    "\n",
    "# Optimized model configuration\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "dense_model = dense_model.half().eval()  # FP16 + inference mode\n",
    "\n",
    "# Switch to CoreML-compatible sparse model\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    providers=[\"CoreMLExecutionProvider\"],\n",
    "    provider_options={\n",
    "        \"MLComputeUnits\": \"CPUAndGPU\",\n",
    "        \"RequireStaticInputShapes\": \"0\",\n",
    "        \"EnableOnSubgraphs\": \"1\"\n",
    "    },\n",
    "    quantize=True  # 4-bit quantization\n",
    ")\n",
    "\n",
    "# Reverse processing configuration\n",
    "batch_size = 512\n",
    "data_points = []\n",
    "total_documents = len(documents)\n",
    "document_indices = reversed(range(total_documents))  # Create reverse index mapping\n",
    "\n",
    "print(f\"\\nTotal documents to process (reverse order): {total_documents}\")\n",
    "\n",
    "# Process in reverse batches\n",
    "for batch_num, idx in enumerate(range(total_documents - 1, -1, -batch_size)):\n",
    "    start_idx = max(0, idx - batch_size + 1)\n",
    "    end_idx = idx + 1\n",
    "    batch_docs = documents[start_idx:end_idx]\n",
    "    \n",
    "    # Reverse batch to maintain original order within chunks\n",
    "    batch_docs = batch_docs[::-1]\n",
    "    batch_indices = range(idx, start_idx - 1, -1)\n",
    "\n",
    "    # Process in smaller chunks\n",
    "    for micro_batch, micro_indices in zip(\n",
    "        np.array_split(batch_docs, 4),\n",
    "        np.array_split(batch_indices, 4)\n",
    "    ):\n",
    "        with torch.inference_mode(), torch.autocast(device_type=device):\n",
    "            dense_vectors = dense_model.encode(micro_batch, convert_to_numpy=True)\n",
    "            sparse_vectors = list(sparse_model.embed(micro_batch))\n",
    "        \n",
    "        # Format with original indices\n",
    "        for i, (doc, dense_vec, sparse_vec) in enumerate(zip(micro_batch, dense_vectors, sparse_vectors)):\n",
    "            original_index = micro_indices[i]\n",
    "            data_points.append({\n",
    "                \"id\": original_index,\n",
    "                \"vector\": {\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"sparse\": {\n",
    "                        \"indices\": sparse_vec.indices.tolist(),\n",
    "                        \"values\": sparse_vec.values.tolist()\n",
    "                    }\n",
    "                },\n",
    "                \"payload\": {\n",
    "                    \"text\": doc,\n",
    "                    \"user_id\": f\"user_{original_index % 10 + 1}\"\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del dense_vectors, sparse_vectors\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Progress tracking\n",
    "    processed = min((batch_num + 1) * batch_size, total_documents)\n",
    "    sys.stdout.write(f\"\\rProcessed: {processed}/{total_documents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = len(documents)\n",
    "print(f\"\\nTotal documents to process: {total_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(data_points[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Create collection With additional index configuration\n",
    "client.create_collection(\n",
    "    collection_name=\"hybrid-search-demo\",\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=384,\n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=models.HnswConfigDiff(\n",
    "                m=16,\n",
    "                ef_construct=100\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "                full_scan_threshold=20000\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=20000,\n",
    "        memmap_threshold=20000\n",
    "    ),\n",
    "    shard_number=3,\n",
    "    replication_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from qdrant_client import models\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Batch processing for better performance\n",
    "batch_size = 500  # Adjust based on your system's memory\n",
    "points_to_upsert = []\n",
    "total_points = len(data_points)\n",
    "batches_processed = 0\n",
    "points_upserted = 0\n",
    "\n",
    "for idx, point in enumerate(data_points):\n",
    "    # Convert sparse vector to Qdrant's required format\n",
    "    sparse_vector = models.SparseVector(\n",
    "        indices=point[\"vector\"][\"sparse\"][\"indices\"],\n",
    "        values=point[\"vector\"][\"sparse\"][\"values\"]\n",
    "    )\n",
    "    \n",
    "    # Create PointStruct with proper vector configuration\n",
    "    points_to_upsert.append(\n",
    "        models.PointStruct(\n",
    "            id=point[\"id\"],\n",
    "            vector={\n",
    "                \"dense\": point[\"vector\"][\"dense\"],\n",
    "                \"sparse\": sparse_vector,\n",
    "            },\n",
    "            payload=point[\"payload\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Upsert in batches\n",
    "    if len(points_to_upsert) >= batch_size or idx == total_points - 1:\n",
    "        client.upsert(\n",
    "            collection_name=\"hybrid-search-demo\",\n",
    "            points=points_to_upsert\n",
    "        )\n",
    "        batches_processed += 1\n",
    "        points_upserted += len(points_to_upsert)\n",
    "        \n",
    "        # Print progress on a single line\n",
    "        sys.stdout.write(f\"\\rBatches processed: {batches_processed}, Points upserted: {points_upserted}/{total_points}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Clear the batch\n",
    "        points_to_upsert = []\n",
    "\n",
    "# Final progress message on a new line\n",
    "print(\"\\nUpserting complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "from fastembed.rerank.cross_encoder import TextCrossEncoder\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Initialize models\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sparse_model = SparseTextEmbedding(\"prithivida/Splade_PP_en_v1\")\n",
    "reranker = TextCrossEncoder(model_name='jinaai/jina-reranker-v2-base-multilingual')\n",
    "\n",
    "def hybrid_search(query: str, user_filter: str = None) -> List[Tuple[float, str]]:\n",
    "    # Generate embeddings\n",
    "    dense_vec = dense_model.encode(query).tolist()\n",
    "    sparse_embedding = next(sparse_model.embed(query))\n",
    "    \n",
    "    # Create Qdrant-compatible sparse vector\n",
    "    sparse_query = models.SparseVector(\n",
    "        indices=sparse_embedding.indices.tolist(),\n",
    "        values=sparse_embedding.values.tolist()\n",
    "    )\n",
    "    \n",
    "    # Build search requests with payload validation\n",
    "    requests = [\n",
    "        models.SearchRequest(\n",
    "            vector=models.NamedVector(\n",
    "                name=\"dense\",\n",
    "                vector=dense_vec\n",
    "            ),\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_filter)\n",
    "                )]\n",
    "            ) if user_filter else None,\n",
    "            limit=100,\n",
    "            with_payload=[\"text\"]  # Explicitly request text field\n",
    "        ),\n",
    "        models.SearchRequest(\n",
    "            vector=models.NamedSparseVector(\n",
    "                name=\"sparse\",\n",
    "                vector=sparse_query\n",
    "            ),\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_filter)\n",
    "                )]\n",
    "            ) if user_filter else None,\n",
    "            limit=100,\n",
    "            with_payload=[\"text\"]  # Explicitly request text field\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Execute search with error handling\n",
    "    try:\n",
    "        results = client.search_batch(\n",
    "            collection_name=\"hybrid-search-demo\",\n",
    "            requests=requests\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    # Combine results with empty check\n",
    "    if len(results) < 2 or not results[0] or not results[1]:\n",
    "        print(\"No results from one or both search types\")\n",
    "        return []\n",
    "\n",
    "    fused = reciprocal_rank_fusion([results[0], results[1]])\n",
    "    \n",
    "    # Validate and prepare documents for reranking\n",
    "    documents = []\n",
    "    for hit in fused:\n",
    "        if hit.payload and \"text\" in hit.payload:\n",
    "            documents.append(hit.payload[\"text\"])\n",
    "        else:\n",
    "            print(f\"Skipping hit {hit.id} with missing text payload\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No valid documents to rerank\")\n",
    "        return []\n",
    "\n",
    "    # Rerank results with type conversion\n",
    "    reranked_scores = list(reranker.rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        k=10\n",
    "    ))\n",
    "\n",
    "    # Pairing scores with their corresponding documents\n",
    "    reranked = [(score, documents[idx]) for idx, score in enumerate(reranked_scores)]\n",
    "\n",
    "    # Sort the results by score (from highest negative to lowest negative score)\n",
    "    reranked_sorted = sorted(reranked, key=lambda x: x[0])\n",
    "\n",
    "    # Return top 20 results\n",
    "    return reranked_sorted[:20]\n",
    "\n",
    "def reciprocal_rank_fusion(results_list: list, k: int = 60) -> list:\n",
    "    \"\"\"Safe RRF implementation with input validation\"\"\"\n",
    "    fused_scores = {}\n",
    "    all_hits = {}\n",
    "    \n",
    "    for results in results_list:\n",
    "        if not isinstance(results, list):\n",
    "            continue\n",
    "            \n",
    "        for rank, hit in enumerate(results, 1):\n",
    "            if not hit.payload or \"text\" not in hit.payload:\n",
    "                continue\n",
    "                \n",
    "            if hit.id not in fused_scores:\n",
    "                fused_scores[hit.id] = 0.0\n",
    "                all_hits[hit.id] = hit\n",
    "            fused_scores[hit.id] += 1.0 / (rank + k)\n",
    "    \n",
    "    return sorted(all_hits.values(), key=lambda x: fused_scores.get(x.id, 0), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "query = \"car crimes happened in may 2009\"\n",
    "user_filter = \"user_2\"  # Example filter by user_id\n",
    "\n",
    "results = hybrid_search(query=query, user_filter=user_filter)\n",
    "\n",
    "for idx, (score, text) in enumerate(results):\n",
    "    print(f\"{idx + 1}. [Score: {score:.2f}] {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
