{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install qdrant-client fastembed sentence-transformers numpy pandas pyarrow colbert  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(32434) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (4.39.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.3\n",
      "    Uninstalling transformers-4.39.3:\n",
      "      Successfully uninstalled transformers-4.39.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.0.2 which is incompatible.\n",
      "fastembed 0.2.0 requires huggingface-hub<0.21,>=0.20, but you have huggingface-hub 0.29.3 which is incompatible.\n",
      "fastembed 0.2.0 requires tokenizers<0.16.0,>=0.15.1, but you have tokenizers 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.29.3 tokenizers-0.21.1 transformers-4.50.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fastembed\n",
      "  Using cached fastembed-0.6.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.29.3)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.7.3)\n",
      "Requirement already satisfied: mmh3<6.0.0,>=4.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (4.1.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.21 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (2.0.2)\n",
      "Requirement already satisfied: onnxruntime<1.20.0,>=1.17.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (1.19.2)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (10.4.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.1.5)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (6.30.1)\n",
      "Requirement already satisfied: sympy in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (1.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (2025.1.31)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from coloredlogs->onnxruntime<1.20.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime<1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
      "Using cached fastembed-0.6.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: fastembed\n",
      "Successfully installed fastembed-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(32333) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers>=3.0.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (3.1.1)\n",
      "Requirement already satisfied: qdrant-client[fastembed] in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (1.13.3)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (1.71.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (1.71.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.28.1)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.21 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (2.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (2.10.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from qdrant-client[fastembed]) (2.3.0)\n",
      "Collecting fastembed==0.5.1 (from qdrant-client[fastembed])\n",
      "  Using cached fastembed-0.5.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.20.3)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.7.3)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (4.1.0)\n",
      "Requirement already satisfied: onnxruntime<1.20.0,>=1.17.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (1.19.2)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (10.4.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.1.5)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=3.0.0) (4.39.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=3.0.0) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=3.0.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=3.0.0) (1.13.1)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from grpcio-tools>=1.41.0->qdrant-client[fastembed]) (5.29.4)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from grpcio-tools>=1.41.0->qdrant-client[fastembed]) (58.0.4)\n",
      "Requirement already satisfied: anyio in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.2.0)\n",
      "Requirement already satisfied: filelock in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (2.27.2)\n",
      "Requirement already satisfied: networkx in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=3.0.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=3.0.0) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=3.0.0) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers>=3.0.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers>=3.0.0) (3.6.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.1.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (25.2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed==0.5.1->qdrant-client[fastembed]) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=3.0.0) (3.0.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/babanakov/Library/Python/3.9/lib/python/site-packages (from coloredlogs->onnxruntime<1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (10.0)\n",
      "Using cached fastembed-0.5.1-py3-none-any.whl (69 kB)\n",
      "Installing collected packages: fastembed\n",
      "  Attempting uninstall: fastembed\n",
      "    Found existing installation: fastembed 0.6.0\n",
      "    Uninstalling fastembed-0.6.0:\n",
      "      Successfully uninstalled fastembed-0.6.0\n",
      "Successfully installed fastembed-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'qdrant-client[fastembed]' 'sentence-transformers>=3.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade 'fastembed>=0.2.0' 'qdrant-client[fastembed]' sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/babanakov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/babanakov/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.sparse import Bm42\n",
    "sparse_model = Bm42()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import fastembed.sparse\n",
    "print(dir(fastembed.sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer warnings\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # M3 GPU fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Load Seattle Police data from official source\n",
    "url = \"~/Downloads/SPD_Crime_Data__2008-Present.csv\"\n",
    "# url = \"https://data.seattle.gov/api/views/tazs-3rd5/rows.csv?accessType=DOWNLOAD\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Column names to use\n",
    "crime_type_col = 'NIBRS Offense Code Description'\n",
    "subcategory_col = 'Offense Sub Category'\n",
    "location_col = 'Neighborhood'\n",
    "date_col = 'Offense Date'\n",
    "precinct_col = 'Precinct'\n",
    "\n",
    "# Filter and prepare documents from police reports\n",
    "df = df.dropna(subset=[crime_type_col, subcategory_col]).reset_index(drop=True)\n",
    "documents = [\n",
    "    f\"Police report {i}: {row[crime_type_col]} - {row[subcategory_col]} \" \n",
    "    f\"at {row[location_col]} on {pd.to_datetime(row[date_col]).strftime('%Y-%m-%d')}\"\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Memory optimization: Clear unused objects\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# Initialize models with GPU optimizations\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.set_per_process_memory_fraction(0.65)  # Increased safety margin\n",
    "\n",
    "# Optimized model configuration\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "dense_model = dense_model.half().eval()  # FP16 + inference mode\n",
    "\n",
    "# Switch to CoreML-compatible sparse model\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    providers=[\"CoreMLExecutionProvider\"],\n",
    "    provider_options={\n",
    "        \"MLComputeUnits\": \"CPUAndGPU\",\n",
    "        \"RequireStaticInputShapes\": \"0\",\n",
    "        \"EnableOnSubgraphs\": \"1\"\n",
    "    },\n",
    "    quantize=True  # 4-bit quantization\n",
    ")\n",
    "\n",
    "# Batch processing configuration (reduced for 8GB RAM)\n",
    "batch_size = 512  # Reduced from 1024\n",
    "data_points = []\n",
    "total_documents = len(documents)\n",
    "print(f\"\\nTotal documents to process: {total_documents}\")\n",
    "\n",
    "for idx in range(0, total_documents, batch_size):\n",
    "    batch_docs = documents[idx:idx+batch_size]\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for micro_batch in np.array_split(batch_docs, 4):\n",
    "        # Generate embeddings with memory cleanup\n",
    "        with torch.inference_mode(), torch.autocast(device_type=device):\n",
    "            dense_vectors = dense_model.encode(micro_batch, convert_to_numpy=True)\n",
    "            sparse_vectors = list(sparse_model.embed(micro_batch))\n",
    "        \n",
    "        # Format for Qdrant\n",
    "        for i, (doc, dense_vec, sparse_vec) in enumerate(zip(micro_batch, dense_vectors, sparse_vectors)):\n",
    "            data_points.append({\n",
    "                \"id\": idx + i,\n",
    "                \"vector\": {\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"sparse\": {\n",
    "                        \"indices\": sparse_vec.indices.tolist(),\n",
    "                        \"values\": sparse_vec.values.tolist()\n",
    "                    }\n",
    "                },\n",
    "                \"payload\": {\n",
    "                    \"text\": doc,\n",
    "                    \"user_id\": f\"user_{(idx + i) % 10 + 1}\"  # Simplified user assignment\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        # Explicit memory cleanup\n",
    "        del dense_vectors, sparse_vectors\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Update status\n",
    "    sys.stdout.write(f\"\\rProcessed: {len(data_points)}/{total_documents} | Batch size: {batch_size} | Mem usage: {torch.mps.current_allocated_memory()/1e6:.1f}MB\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n\\nProcessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Initialize models with GPU optimizations\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.set_per_process_memory_fraction(0.65)  # Increased safety margin\n",
    "\n",
    "# Optimized model configuration\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "dense_model = dense_model.half().eval()  # FP16 + inference mode\n",
    "\n",
    "# Switch to CoreML-compatible sparse model\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    providers=[\"CoreMLExecutionProvider\"],\n",
    "    provider_options={\n",
    "        \"MLComputeUnits\": \"CPUAndGPU\",\n",
    "        \"RequireStaticInputShapes\": \"0\",\n",
    "        \"EnableOnSubgraphs\": \"1\"\n",
    "    },\n",
    "    quantize=True  # 4-bit quantization\n",
    ")\n",
    "\n",
    "# Reverse processing configuration\n",
    "batch_size = 512\n",
    "data_points = []\n",
    "total_documents = len(documents)\n",
    "document_indices = reversed(range(total_documents))  # Create reverse index mapping\n",
    "\n",
    "print(f\"\\nTotal documents to process (reverse order): {total_documents}\")\n",
    "\n",
    "# Process in reverse batches\n",
    "for batch_num, idx in enumerate(range(total_documents - 1, -1, -batch_size)):\n",
    "    start_idx = max(0, idx - batch_size + 1)\n",
    "    end_idx = idx + 1\n",
    "    batch_docs = documents[start_idx:end_idx]\n",
    "    \n",
    "    # Reverse batch to maintain original order within chunks\n",
    "    batch_docs = batch_docs[::-1]\n",
    "    batch_indices = range(idx, start_idx - 1, -1)\n",
    "\n",
    "    # Process in smaller chunks\n",
    "    for micro_batch, micro_indices in zip(\n",
    "        np.array_split(batch_docs, 4),\n",
    "        np.array_split(batch_indices, 4)\n",
    "    ):\n",
    "        with torch.inference_mode(), torch.autocast(device_type=device):\n",
    "            dense_vectors = dense_model.encode(micro_batch, convert_to_numpy=True)\n",
    "            sparse_vectors = list(sparse_model.embed(micro_batch))\n",
    "        \n",
    "        # Format with original indices\n",
    "        for i, (doc, dense_vec, sparse_vec) in enumerate(zip(micro_batch, dense_vectors, sparse_vectors)):\n",
    "            original_index = micro_indices[i]\n",
    "            data_points.append({\n",
    "                \"id\": original_index,\n",
    "                \"vector\": {\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"sparse\": {\n",
    "                        \"indices\": sparse_vec.indices.tolist(),\n",
    "                        \"values\": sparse_vec.values.tolist()\n",
    "                    }\n",
    "                },\n",
    "                \"payload\": {\n",
    "                    \"text\": doc,\n",
    "                    \"user_id\": f\"user_{original_index % 10 + 1}\"\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del dense_vectors, sparse_vectors\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Progress tracking\n",
    "    processed = min((batch_num + 1) * batch_size, total_documents)\n",
    "    sys.stdout.write(f\"\\rProcessed: {processed}/{total_documents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = len(documents)\n",
    "print(f\"\\nTotal documents to process: {total_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(data_points[0], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "df = pd.DataFrame(data_points)\n",
    "\n",
    "# Explode nested structures for Parquet compatibility\n",
    "df = pd.json_normalize(df.to_dict(orient='records'))\n",
    "\n",
    "# Handle file path expansion\n",
    "output_path = os.path.expanduser(\"~/Downloads/seattle_police_data.parquet\")\n",
    "\n",
    "# Convert to PyArrow Table with proper schema\n",
    "schema = pa.schema([\n",
    "    (\"id\", pa.int64()),\n",
    "    (\"vector.dense\", pa.list_(pa.float32())),\n",
    "    (\"vector.sparse.indices\", pa.list_(pa.int64())),\n",
    "    (\"vector.sparse.values\", pa.list_(pa.float32())),\n",
    "    (\"payload.text\", pa.string()),\n",
    "    (\"payload.user_id\", pa.string())\n",
    "])\n",
    "\n",
    "table = pa.Table.from_pandas(df, schema=schema)\n",
    "\n",
    "# Write with compression\n",
    "pq.write_table(\n",
    "    table,\n",
    "    output_path,\n",
    "    compression='SNAPPY',\n",
    "    use_dictionary=True\n",
    ")\n",
    "\n",
    "print(f\"Parquet file saved to: {output_path}\")\n",
    "print(f\"Size: {os.path.getsize(output_path)/1024/1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Create collection With additional index configuration\n",
    "client.create_collection(\n",
    "    collection_name=\"hybrid-search-demo\",\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=384,\n",
    "            distance=models.Distance.COSINE,\n",
    "            hnsw_config=models.HnswConfigDiff(\n",
    "                m=16,\n",
    "                ef_construct=100\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "                full_scan_threshold=20000\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=20000,\n",
    "        memmap_threshold=20000\n",
    "    ),\n",
    "    shard_number=3,\n",
    "    replication_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from qdrant_client import models\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Batch processing for better performance\n",
    "batch_size = 500  # Adjust based on your system's memory\n",
    "points_to_upsert = []\n",
    "total_points = len(data_points)\n",
    "batches_processed = 0\n",
    "points_upserted = 0\n",
    "\n",
    "for idx, point in enumerate(data_points):\n",
    "    # Convert sparse vector to Qdrant's required format\n",
    "    sparse_vector = models.SparseVector(\n",
    "        indices=point[\"vector\"][\"sparse\"][\"indices\"],\n",
    "        values=point[\"vector\"][\"sparse\"][\"values\"]\n",
    "    )\n",
    "    \n",
    "    # Create PointStruct with proper vector configuration\n",
    "    points_to_upsert.append(\n",
    "        models.PointStruct(\n",
    "            id=point[\"id\"],\n",
    "            vector={\n",
    "                \"dense\": point[\"vector\"][\"dense\"],\n",
    "                \"sparse\": sparse_vector,\n",
    "            },\n",
    "            payload=point[\"payload\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Upsert in batches\n",
    "    if len(points_to_upsert) >= batch_size or idx == total_points - 1:\n",
    "        client.upsert(\n",
    "            collection_name=\"hybrid-search-demo\",\n",
    "            points=points_to_upsert\n",
    "        )\n",
    "        batches_processed += 1\n",
    "        points_upserted += len(points_to_upsert)\n",
    "        \n",
    "        # Print progress on a single line\n",
    "        sys.stdout.write(f\"\\rBatches processed: {batches_processed}, Points upserted: {points_upserted}/{total_points}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Clear the batch\n",
    "        points_to_upsert = []\n",
    "\n",
    "# Final progress message on a new line\n",
    "print(\"\\nUpserting complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed.sparse import SparseTextEmbedding\n",
    "from fastembed.rerank.cross_encoder import TextCrossEncoder\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Initialize models\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"mps\")\n",
    "sparse_model = SparseTextEmbedding(\"prithivida/Splade_PP_en_v1\")\n",
    "reranker = TextCrossEncoder(model_name='jinaai/jina-reranker-v2-base-multilingual')\n",
    "\n",
    "def hybrid_search(query: str, user_filter: str = None) -> List[Tuple[float, str]]:\n",
    "    # Generate embeddings\n",
    "    dense_vec = dense_model.encode(query).tolist()\n",
    "    sparse_embedding = next(sparse_model.embed(query))\n",
    "    \n",
    "    # Create Qdrant-compatible sparse vector\n",
    "    sparse_query = models.SparseVector(\n",
    "        indices=sparse_embedding.indices.tolist(),\n",
    "        values=sparse_embedding.values.tolist()\n",
    "    )\n",
    "    \n",
    "    # Build search requests with payload validation\n",
    "    requests = [\n",
    "        models.SearchRequest(\n",
    "            vector=models.NamedVector(\n",
    "                name=\"dense\",\n",
    "                vector=dense_vec\n",
    "            ),\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_filter)\n",
    "                )]\n",
    "            ) if user_filter else None,\n",
    "            limit=100,\n",
    "            with_payload=[\"text\"]  # Explicitly request text field\n",
    "        ),\n",
    "        models.SearchRequest(\n",
    "            vector=models.NamedSparseVector(\n",
    "                name=\"sparse\",\n",
    "                vector=sparse_query\n",
    "            ),\n",
    "            filter=models.Filter(\n",
    "                must=[models.FieldCondition(\n",
    "                    key=\"user_id\",\n",
    "                    match=models.MatchValue(value=user_filter)\n",
    "                )]\n",
    "            ) if user_filter else None,\n",
    "            limit=100,\n",
    "            with_payload=[\"text\"]  # Explicitly request text field\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Execute search with error handling\n",
    "    try:\n",
    "        results = client.search_batch(\n",
    "            collection_name=\"hybrid-search-demo\",\n",
    "            requests=requests\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    # Combine results with empty check\n",
    "    if len(results) < 2 or not results[0] or not results[1]:\n",
    "        print(\"No results from one or both search types\")\n",
    "        return []\n",
    "\n",
    "    fused = reciprocal_rank_fusion([results[0], results[1]])\n",
    "    \n",
    "    # Validate and prepare documents for reranking\n",
    "    documents = []\n",
    "    for hit in fused:\n",
    "        if hit.payload and \"text\" in hit.payload:\n",
    "            documents.append(hit.payload[\"text\"])\n",
    "        else:\n",
    "            print(f\"Skipping hit {hit.id} with missing text payload\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No valid documents to rerank\")\n",
    "        return []\n",
    "\n",
    "    # Rerank results with type conversion\n",
    "    reranked_scores = list(reranker.rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        k=10\n",
    "    ))\n",
    "\n",
    "    # Pairing scores with their corresponding documents\n",
    "    reranked = [(score, documents[idx]) for idx, score in enumerate(reranked_scores)]\n",
    "\n",
    "    # Sort the results by score (from highest negative to lowest negative score)\n",
    "    reranked_sorted = sorted(reranked, key=lambda x: x[0])\n",
    "\n",
    "    # Return top 20 results\n",
    "    return reranked_sorted[:20]\n",
    "\n",
    "def reciprocal_rank_fusion(results_list: list, k: int = 60) -> list:\n",
    "    \"\"\"Safe RRF implementation with input validation\"\"\"\n",
    "    fused_scores = {}\n",
    "    all_hits = {}\n",
    "    \n",
    "    for results in results_list:\n",
    "        if not isinstance(results, list):\n",
    "            continue\n",
    "            \n",
    "        for rank, hit in enumerate(results, 1):\n",
    "            if not hit.payload or \"text\" not in hit.payload:\n",
    "                continue\n",
    "                \n",
    "            if hit.id not in fused_scores:\n",
    "                fused_scores[hit.id] = 0.0\n",
    "                all_hits[hit.id] = hit\n",
    "            fused_scores[hit.id] += 1.0 / (rank + k)\n",
    "    \n",
    "    return sorted(all_hits.values(), key=lambda x: fused_scores.get(x.id, 0), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/m52s16jx53xdncp455ty9ps00000gq/T/ipykernel_32635/353891791.py:63: DeprecationWarning: `search_batch` method is deprecated and will be removed in the future. Use `query_batch_points` instead.\n",
      "  results = client.search_batch(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [Score: -2.13] Police report 188398: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-01-18\n",
      "2. [Score: -2.13] Police report 188382: Theft From Motor Vehicle - LARCENY-THEFT at - on 2015-12-03\n",
      "3. [Score: -2.13] Police report 188382: Theft From Motor Vehicle - LARCENY-THEFT at - on 2015-12-03\n",
      "4. [Score: -2.13] Police report 179672: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-02-16\n",
      "5. [Score: -2.13] Police report 179672: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-02-16\n",
      "6. [Score: -2.12] Police report 193905: Theft From Motor Vehicle - LARCENY-THEFT at - on 2016-02-16\n",
      "7. [Score: -2.11] Police report 193987: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-11-20\n",
      "8. [Score: -2.11] Police report 193987: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-11-20\n",
      "9. [Score: -2.11] Police report 177372: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-10-06\n",
      "10. [Score: -2.10] Police report 189794: Theft From Motor Vehicle - LARCENY-THEFT at - on 2009-07-08\n",
      "11. [Score: -2.10] Police report 193792: Theft From Motor Vehicle - LARCENY-THEFT at - on 2014-06-07\n",
      "12. [Score: -2.10] Police report 18943: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-08-02\n",
      "13. [Score: -2.10] Police report 18943: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-08-02\n",
      "14. [Score: -2.10] Police report 188386: Theft From Motor Vehicle - LARCENY-THEFT at - on 2014-11-21\n",
      "15. [Score: -2.10] Police report 188333: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-06-13\n",
      "16. [Score: -2.10] Police report 357331: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-02-22\n",
      "17. [Score: -2.10] Police report 189942: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-05-07\n",
      "18. [Score: -2.10] Police report 189942: Theft From Motor Vehicle - LARCENY-THEFT at - on 2008-05-07\n",
      "19. [Score: -2.09] Police report 193798: Theft From Motor Vehicle - LARCENY-THEFT at - on 2010-05-04\n",
      "20. [Score: -2.09] Police report 188388: Theft From Motor Vehicle - LARCENY-THEFT at - on 2014-03-14\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "query = \"Motor Vehicle and violent\"\n",
    "user_filter = \"\"  # Example filter by user_id\n",
    "\n",
    "results = hybrid_search(query=query, user_filter=user_filter)\n",
    "\n",
    "for idx, (score, text) in enumerate(results):\n",
    "    print(f\"{idx + 1}. [Score: {score:.2f}] {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
